\label{sec:daq}
The HPS experiment data acquisition (DAQ) and trigger system handles the acquisition of data and 
generation and distribution of triggers for the three sub-detectors: the SVT, ECal and the Muon System. 
HPS employs two DAQ architectures: the SVT is readout with 
Advanced Telecom Communications Architecture~\cite{atca} (ATCA) hardware
while the ECal and Muon System use VXS based hardware. The Level~1 trigger receives input from the 
ECal and Muon system, and is distributed to all the detector systems to signal event 
selection. Figure~\ref{fig:daq_hardware_overview} gives a schematic block diagram of the DAQ system.
\begin{figure*}[t]
%\includegraphics[ scale=0.25]{test2012/ecal_mounted.JPG}
\includegraphics[ scale=0.3]{daq_trigger/figures/daq_schem.pdf}
\caption{\small{Schematic block diagram of the data acquisition system.}}
\label{fig:daq_hardware_overview}
\end{figure*}

For the ECal and Muon System, every VXS or VME crate contains a Readout Controller (ROC) that collects digitized information, 
processes it, and sends it on to the
Event Builder. The ROC is the single blade Intel-based CPU module running DAQ software under CentOS Linux OS. 
For the SVT ATCA system the ROC application runs on an embedded 
processor contained on the ATCA COB.
The Event Builder assembles 
information from the SVT, ECal and Muon System ROCs into a single 
event which is passed to the Event Recorder, which in turn writes it to a data storage system capable of handling up to 100MBytes/s.
The Event Builder and other 
critical components run on multicore Intel-based multi-CPU servers. The backbone of the DAQ network system is a 
Foundry router providing 1Gbit and 10Gbit connections between DAQ components and 
the JLab computing facility. The SVT ROC, which must handle large data volumes, has a 10Gbit link to the Foundry router, while the 
ROCs for the ECal and Muon System connect through a 1Gbit network switch. The long 
term data storage is handled by a 10Gbit uplink to the JLab computing facility. 

Section~\ref{sec:svt_daq} describes the SVT DAQ in more detail. The VXS based readout for 
the ECal and muon detector is described in Sec.~\ref{sec:fadc_daq} and the trigger 
system is explained in more detail in Sec.~\ref{sec:triggerdaq}.

\subsubsection{SVT Data Acquisition}
\label{sec:svt_daq}
The goal of the SVT DAQ is to support the continuous 40~MHz readout and processing of signals from 
the 36 silicon strip sensors of the SVT and to select those events that were identified by the 
Level 1 trigger system for transfer to the JLab DAQ for further event processing at rates 
close to 50~kHz. High data volumes result from high occupancy in the detector, pile-up from multiple bunches,
and sampling pulse heights in six consecutive time buckets for each hit in order to facilitate 
reconstruction of the hit time to high accuracy.

The system adopted is an evolution of the SVT DAQ used for the HPS Test Run. Several features changed in
response to the new SVT design and the evolution of SLAC's ATCA system. The new SVT has nearly twice 
the number of sensors as the Test Run detector, 
necessitating a more compact way to transfer data and power to the individual sensor modules to the
vacuum flanges. Accordingly, the new system incorporates a front end board within the vacuum volume for 
power distribution and signal digitization,
allowing many fewer vacuum connections per sensor and less interference within the vacuum volume. 
Problems encountered with reflections on long twisted pair
data lines, although ultimately overcome, have been avoided altogether by incorporating a flange board just 
outside of the vacuum and adopting an optical link. The ATCA system has evolved to using optical input, so 
this change lets HPS optimally piggyback on SLAC's ATCA system development.  

Each of the 36 silicon strip sensors is connected to a 
hybrid board incorporating five 128-channel APV25 front-end 
ASICs~\cite{Jones:1069892,Raymond:2002yr}, see Fig.~\ref{fig:hybrid_and_apv25_testrun}.
The APV25 ASIC, initially developed for the Compact Muon Solenoid silicon tracker  at the Large Hadron 
Collider at CERN, was chosen because it provides excellent signal to noise, analog output for optimal 
spatial resolution, and multi-bucket output for good time resolution. 
Each hybrid board has five analog output lines (one for each of the APV25 ASICs) which are sent to the  
front-end readout board using low power LVDS differential current signals over about 1~m of flex cable.
At the readout board, a preamplifier scales the APV25 differential current output to match 
the range of a 14-bit Analog to Digital Converter (ADC). Each front-end board services four hybrids. 
The ADC operates at the system clock frequency of 41.667~MHz.
The digitized output from the front-end board is sent through compact 8-pair mini-SAS cables to 
the vacuum flanges to connect to the external DAQ which resides outside the vacuum chamber. 
The front-end readout board houses a FPGA and buffers to allow for the control of the 
distribution of clock, trigger and I$^{2}$C communication with the APV25 ASICs. 
To further simplify the services and minimize cabling that enter through the vacuum flanges, it contains 
linear regulators to distribute and regulate three low voltage power lines to the APV25 ASICs and
the high voltage bias. Figure~\ref{fig:svt_daq_flange_fe_boards} shows a schematics layout of the 
downstream readout chain of the SVT.
 \begin{figure*}[]
\includegraphics[ scale=0.6]{daq_trigger/figures/daq_hps_2014_schematics_fe_flange_boards.pdf} 
\caption{\small{Schematic overview of the front end and flange boards of the downstream part of 
SVT DAQ.}}
\label{fig:svt_daq_flange_fe_boards}
\end{figure*}

The digitized signals are converted to optical signals just outside the vacuum flange on custom built  
flange boards. Each flange board houses optical drivers to handle the electrical-optical 
conversion and to transmit the optical signals over $\sim 10$~m fibers to the SVT DAQ. 
The flange board also interfaces the low- and high voltage power transmission from the CAEN power 
supplies to the vacuum chamber.  

The main SVT DAQ uses the ATCA system for high speed data transfer. The optical signals from four hybrids, 
one half flange board, are received at one of four sections of the Rear Transition Module (RTM) boards of the 
ATCA crate, see Fig.~\ref{fig:svt_daq_overview}.
\begin{figure*}[]
\includegraphics[ scale=0.6]{daq_trigger/figures/daq_hps_2014_schematics_atca.pdf} 
\caption{\small{Schematic block diagrams of the SVT data acquisition system.}}
\label{fig:svt_daq_overview}
\end{figure*}
Each section of the RTM connects to one of four FPGA units on the main ATCA board, 
the COB (Cluster On Board). Each FPGA is housed on a 
separate daughter board called a Data Processing Module (DPM). The modular ATCA design permits the 
HPS DAQ to re-use architecture and functionality from other DAQ system such as the ATLAS muon 
system which components are similar to that used by HPS. Figure~\ref{fig:rtm_testrun} shows the boards designed and used for the HPS test.
In order to minimize the complexity of the system inside the vacuum chamber, all signal processing is 
done at the DPM.  Each DPM receives the digitized signals 
from the RTM, applies thresholds for data reduction and organizes the sample data 
into UDP datagrams. One of the DPMs functions as the trigger interface which receives trigger 
signals from the optical fiber module on the RTM, distributes clock and trigger signals, 
and handles communication with the JLab trigger supervisor and the RCE. The 
RCE (Reconfigurable Cluster Element) is a generic computational building block 
on the trigger interface DPM running a 450~MHz PPC processor with 2~GB of DDR3 
memory. Four COBs housed in a two ATCA crates is sufficient to handle 
the 36 hybrids of the SVT.

The RCE receives and buffers UDP datagrams from the data and trigger DPMs and
 assembles them into full event frames. The RCE also runs an implementation of the JLab ROC application 
that integrates the SVT event frames into the JLab DAQ 
 system described above. The RCE node transfers data to the JLab DAQ  
 through a 10~Gbit Ethernet backend interface. The maximum readout rate of the SVT is approximately 
~50 kHz, limited by the APV25 readout rate. 
%The maximum readout rate of the SVT DAQ  is limited by the readout time 
%of the APV25 chip. Using overlapping trigger and readout functionality, where the 
%APV25 chip can buffer up to 5 triggers, the maximum average readout rate expected for 
%HPS is 45kHz {\color{red} need verification}.   








\subsubsection{ECal and Muon Detector FADC Readout}
\label{sec:fadc_daq}
The readout of the ECal and Muon System are essentially identical. Signals from each 
readout channel are sent to a signal splitter. For the ECal the signals from the APDs are 
shaped and amplified as described in Sec.~\ref{sec:ecal}, then fed into the 
splitter. For the Muon System, signals arise from phototubes recording scintillation light.
One of the outputs of the splitter is input to a 
JLab Flash ADC250 (FADC) channel, one of 16 in a FADC VXS module (see Fig.~\ref{fig:fadc}).
\begin{figure}[]
\includegraphics[scale=0.5]{daq_trigger/figures/FADC250_Photo_001.jpg}
\caption{\small{Jefferson Lab FADC250 VXS module.}}
\label{fig:fadc}
\end{figure}


Two 20-slot VXS crates are used to accommodate the two ECal 
halves, each one with 221 channels, and one 20-slot VXS crate is used to readout the muon 
detector. 

The FADCs store 12-bit digitized samples at 250 MHz in 8~$\mu$s deep pipelines. 
When a trigger is received, the appropriate part of the pipeline is accessed. If an FADC   
signal exceeds a predefined threshold within that time window, the time the signal passed threshold and the integrated
amplitude of X number of samples before and Y samples after the trigger time (X and Y can be varied in the readout code), 
converted into units of deposited energy, are recorded. This data record significantly compactifies the  
input FADC data. During data analysis, a pedestal value is subtracted to obtain actual summed energy.

The FADCs are an integral part of the HPS calorimeter trigger system. Energies  
and times from each FADC channel in the same VXS crate are input to the crate trigger
processor board (CTP) which groups adjacent channels with energy deposited into ``clusters,'' identifies the associated channels, and
assigns an overall cluster energy. Clusters  
from each CTP (one for the top ECal, one for the bottom) are combined in the sub-system processor module (SSP) 
which applies further selection criteria. Events with cluster combinations which pass the criteria are
passed to the trigger supervisor. The trigger process has a pulse timing resolution of 4~ns. This allows a narrow 
coincidence window of 8~ns to be used when searching for clusters, and reduces accidental and out of time coincidences in the ECal 
and the other systems. 

The main characteristics of the FADC are:
\begin{itemize}
\item 12-bit digitizer operating at 250~MHz, 
\item 50$\Omega$ termination input, 
\item front-end input range:  -0.5~V, -1~V or -2~V (the input range must be above the maximum pulse height to 
ensure no signal clipping).
\end{itemize}
The FADC charge resolution as a function of the front-end input range is presented in 
Tab.~\ref{tab:charge_resolution}.
\begin{table}[h]
\centering
\begin{tabular}{|c|c|}
\hline
Input range & Nominal charge resolution\\
(V) & (fC per ADC count)\\\hline
-0.5 & 9.76  \\\hline
-1.0 & 19.53  \\\hline
-2.0 & 39.06 \\\hline
\end{tabular}
\caption{FADC charge resolution for different front-end input ranges.}
\label{tab:charge_resolution}
\end{table}
There are two FADC operation modes: the readout mode and trigger mode.
FADC data paths for the readout and trigger operation are presented in Fig.~\ref{fig:hps_trigger_data}.
\begin{figure}[t]
\includegraphics[scale=0.4]{daq_trigger/figures/hps_trigger_data}
\caption{\small{FADC data paths}}
\label{fig:hps_trigger_data}
\end{figure}
In readout mode, the FADC determines the energy of the one ECal channel that will be reported. 
The channel integration occurs only if the input signal crosses the programmable threshold level.  
Then a programmable number of samples around the threshold crossing are added together to form the 
reported integral.  The readout  mode has the following parameters for every FADC channel (see 
Fig.~\ref{fig:hps_trigger_data}, top panel):
 \begin{itemize}
 \item number of samples integrated before the threshold crossing (NSB), 
 \item number of samples integrated after the  threshold crossing (NSA),
 \item readout threshold, measured in ADC counts.
 \end{itemize}
The number of samples for a given channel integration is the sum of NSB+NSA samples that will be stored in  
 the 17-bit FADC register. It is a fixed gate width pulse integration and there is no pedestal subtraction in the 
 sum (pedestal subtraction happens offline).
 

 


A block diagram of the HPS  trigger processing is shown in Fig.~\ref{fig:hps_trigger_data}, bottom panel. 
The trigger processing mode has the following parameters for every FADC channel:
 \begin{itemize}
 \item number of samples integrated before the threshold crossing (NSB),
 \item number of samples integrated after the  threshold crossing (NSA),
 \item readout threshold, measured in ADC counts, 
 \item pedestal, 
 \item conversion factor (gain) that converts  ADC channel to MeV, from 0 to 8191 MeV, 13 bits, 
 \item energy discriminator (minimum energy cutoff).
 \end{itemize}
The parameters NSB, NSA and readout threshold are the same as in the readout mode.
The pedestal value is then subtracted from the integrated sum over NSB+NSA samples and this value is 
converted to MeV units using the gain conversion factor. The energy can be discriminated to cut off low 
energy pulses before reporting to the CTP. 
The value reported to the CTP is a 13-bit pulse energy and the time the pulse crossed the readout threshold. 
Data for every channel is sent to the CTP every 32~ns (if there is no hit a 0 energy pulse is sent). 
This sets a worst case double pulse resolution of 32~ns per channel, but it can be less than this if pulses occur in adjacent 32ns windows.


See Sec.~\ref{sec:triggerdaq} below for more details on the operation of the trigger system.






\subsubsection{Trigger System}
\label{sec:triggerdaq}
\input{daq_trigger/trigger}

\subsubsection{Event Size and Data Rates}

The high occupancies in the detector requires a high readout bandwidth to be able to transfer hits from the 
detectors to disk. The event sizes and rates are based on estimates from full Geant4-based simulations 
including all known backgrounds. As expected the SVT dominate the expected rates. 
The noise hit occupancy in the SVT is kept low by requiring that three of the six samples are above an
effective threshold of three times the noise level. The dominant contribution to the occupancy is instead 
the high rate of beam background hits estimated. This is estimated 
from detailed full simulation resulting in an occupancy of around 0.3\% or an average of 61 channels above threshold.  
%Background studies (see Sec.~\ref{sec:hps_perf}) show that 
%there are on average 10 tracks per event at a beam energy of 2.2~GeV and current of 
%200~nA. With each track 
%having on average 2 strips above threshold for each sensor there are on average 160 channels above threshold. Each of these channels will result in six digitized samples of the 
%pulse shape giving in total of 1084 samples per event for the SVT.
Each SVT channel has, in addition to the six digitized samples,  header information that identifies the 
the channel number and it's chip address. The complete SVT event size also 
include the overhead from each FPGA and the JLab data stream bank header.  
The maximum average event size increased with decreasing beam energy since a larger 
fraction of backgrounds get larger opening angles and thus potentially higher than the 15~mrad 
vertical dead zone angle. For a beam energy of 1.1~GeV, the average SVT event size is 2.5~kB and 
the rate is 43~MB/s, well within the SVT DAQ capabilities. 
The ECal and muon detectors, with occupancies between 3-10\%, each contribute with an event size of 
approximately 0.3i~kB and maximum rates of about 12~MB/s for the 1.1~GeV run. 
%Each calorimeter or muon hit consist of 8 bytes (4 byte energy, 4 byte time)
 %with a 12 byte header (4 byte trigger number, 8 byte trigger time) for each FADC board. 
 Such rates are well within the 100~MB/s limit for each VXS crate used in the ECal and muon 
DAQ system.
 % for both the The main limitation is of the order of 100Mbytes/s from each VXS crate. For a 
% 10\% occupancy estimated in Sec.~\ref{sec:trig_rate} the ECal event size is approximately 0.7~kbytes which translates to a total data rate of approximately 31.5~Mbytes/s 
%(split between the two VXS crates), well within the DAQ system design. 
%The contribution from the muon system is small due to it's significantly lower number of channels. The system is readout by nine FADC boards in a single VXS crate. The event 
%size for a 10\% occupancy level is 0.2~kbytes which translates to a data rate of 10~Mbytes/s. 
Table~\ref{tab:data_rates} summarizes the event size and data rates. The highest overall rate, for a 1.1~GeV run, and that needs to be written to disk is 56~MB/s which is within the current 
DAQ system design limit of 100~MB/s. 
\begin{table}[]
\centering
\begin{tabular}{|l|ccc|ccc|ccc|}
\hline
 & \multicolumn{3}{|c|}{Occupancy(\%)} &  \multicolumn{3}{|c|}{Event size (kB)} &  \multicolumn{3}{|c|}{Data rate (MB/s)} \\
\hline
Beam energy (GeV) & 1.1 & 2.2 & 6.6 & 1.1 & 2.2 & 6.6 & 1.1 & 2.2 & 6.6 \\
\hline
SVT & 0.5 & 0.3  & 0.3  & 2.5 & 1.7 & 1.5 & 43.1 & 27.2 & 18.9\\
ECal & 3.0 & 4.2  & 4.7 & 0.3 & 0.3  & 0.3 & 12.1 & 4.8  & 3.9 \\
Muon & 10 &  10 & 10  & 0.3 & 0.3 & 0.3 & 5.5 & 4.9 & 3.9 \\
\hline
Total& \multicolumn{3}{|c|}{-} & 3.0 & 2.3 & 2.1 & 53.6 & 36.9 & 26.8 \\
\hline
\end{tabular}
\caption{{\small Summary of the occupancy, event size and data rate expected for the runs at  runs at the three beam 
energies in the run plan. }}
\label{tab:data_rates}
\end{table}
