
The following is an outline of the offline computing model envisioned for satisfying the analysis needs of the HPS experiment. The raw data collected over the running periods must be processed through calibration passes, reconstructed into calibrated objects useful for analysis and separated into analysis streams. Corresponding Monte Carlo will also need to be produced and separated into the same analysis streams.

Table~\ref{tab:data_volume} shows the expected number of triggered events and
%the average trigger rate and 
the total amount of data expected over the 
different runs. For the six weeks long run in 2014 we expect to collect approximately 210~TB of raw data. 
We assume that the experiment collects data for all of its available beam time and the time allocated for detector commissioning, even though the experiment reach only assumes 50\% availability; this gives a conservative estimate of computing requirements. 
Trigger rate estimates are taken from \ref{sec:performance}.
\begin{table}[]
\centering
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
Run & $E_{beam}$ (GeV) & Time (days) & Events ($\times 10^9$) & Raw data (TB) & Processed data (TB)\\
\hline
2014 & 1.1 & 21 & 42 & 127 & 567 \\
2014 & 2.2 & 21 & 38 & 83 & 371  \\
\hline
Total & - & 42 & 80 & 210 & 938 \\
\hline
2015 & 2.2 & 35 & 63 & 138 & 618 \\
2015 & 6.6 & 35 & 53 & 106 & 476 \\
\hline
Total & - & 70 & 116 & 245 & 1094 \\
\hline
\end{tabular}
\caption{{\small Summary of the raw and processed data expected from the HPS runs. }}
\label{tab:data_volume}
\end{table}

The raw data must be processed to produce physics data objects that can be analyzed. This reconstruction process will also include filters to select events of physics interest. We use the event size estimates 
in Table~\ref{tab:raw_data_size}, which are based on Table~\ref{tab:data_rates} from the previous section. 
\begin{table}[]
\centering
\begin{tabular}{|l|c|c|}
\hline
Beam energy & Raw (EVIO) event size (kB) & Reconstructed (LCIO) event size (kB) \\ 
\hline
1.1 GeV  &  3.0 & 13.4 \\
2.2 GeV  &  2.2 & 9.8 \\
6.6 GeV  &  2.0 & 8.9 \\
\hline
\end{tabular}
\caption{{\small Data event sizes. }}
\label{tab:raw_data_size}
\end{table}

For modeling signals, estimating backgrounds and confirming the understanding of the detector 
performance, extensive Monte Carlo simulation is needed. Table~\ref{tab:mc_event_size} summarizes 
the typical event size at the various stages of the simulation. 
\begin{table}[]
\centering
\begin{tabular}{|lccc|}
\hline
Event type & Sim. stage & Size/triggered event (kB) & Mass points  \\
\hline
Beam bkg.  & evgen	& 37.0	& 1	\\
A' signal & evgen	& 0.5	& 10	\\
A'+beam bkg & evgen	& 37.4	& 10	\\
\hline
Beam bkg.  & MC output	& 79.5	& 1	\\
A' signal & MC output	& 2.5	& 10	\\
A'+beam bkg & MC output	& 82.0	& 10	\\
\hline
\end{tabular}
\caption{{\small Event sizes in kB per triggered event, including pileup events for beam background. }}
\label{tab:mc_event_size}
\end{table}

In total approximately 1/10th of the number of data events collected need to be simulated. For the 2014 run this amounts to 8.0 billion events.
In addition, 100 million A' events must be simulated at each of 10 mass points at each beam energy. 
This totals 2 billion A' events in each of 2014 and 2015, assuming that changes in running conditions prevent us from using the same simulated data set for both 2.2 GeV run periods.

In total 2324 (2935)~Tb of storage for data is needed for the 2014 (2015) run.

Tape is currently the only economical storage
solution for storing all of the raw, simulated and processed data.
The processing of the raw data is foreseen to occur at JLab. Given a
typical bandwidth between sites of 3 to 4 Tb/day, analyses needing
access to the hit level information will need to be run at JLab or run
on small samples of exported data unless they can take advantage of the
analysis streams. Analysis streams of events satisfying
pre-selections criteria for targeted analyses will be exported to remote
sites. Likewise, the simulated data is foreseen to be processed JLab
and the equivalent analysis streams for the simulated data
will be used for sharing among the sites.
The HPS storage requirements are summarized in Tab.~\ref{tab:datastorage}.
\begin{table}[tbp]
\centering
\begin{tabular}{|l|c|c|}
\hline
Storage category & 2014 (TB) & 2015 (TB) \\
\hline
Raw data & 210 & 245 \\
Processed raw data & 938 & 1094 \\
Simulated data & 1176 & 1596 \\
\hline
Total tape space & 2324 & 2935 \\
\hline
\end{tabular}
\caption{{\small Data storage summary.}}
\label{tab:datastorage}
\end{table}

In addition, disk space will be 
needed for ntuples, code releases, and scratch areas, as well as staging data being written to or read from tape. This is estimated at roughly 100 TB.

Approximately 0.1~CPU seconds are needed to reconstruct a data event on 
a typical 2.4~GHz core. This would require a total of 2.2 million CPU hours of processing for the 
entire 2014 dataset at the JLab processing center.  To simulate events, approximately 0.02 CPU seconds 
are needed for a beam event and approximately 0.7 seconds for an A' event. In total 9.8 million CPU hours are needed for Monte Carlo 
simulation for the 2014 run. 
Based on experience with previous experiments, it is reasonable to estimate that the net CPU needed for 
analysis work (batch and interactive) will be comparable to that needed for production. 
The HPS offline computing requirements 
are summarized in Tab.~\ref{tab:computing}.
\begin{table}[tbp]
\centering
\begin{tabular}{|l|c|c|}
\hline
Computing category & 2014& 2015 \\
\hline
Raw data processing ($\times 10^{6}$~CPUh)  & 2.2 & 3.2 \\
Simulation production ($\times 10^{6}$~CPUh) & 9.8 & 11.8 \\
\hline
Total ($\times 10^{6}$~CPUh) & 12.0 & 15.0 \\
\hline
\end{tabular}
\caption{{\small Computing needs summary.}}
\label{tab:computing}
\end{table}

The Jefferson Lab Computing Center provides computing and storage for experiments at JLab. 
A request will be submitted for data storage (tape and disk), computing resources (CPU hours for simulation and production), and data transfers to/from JLab.
