
The following is an outline of the offline computing model envisioned for satisfying the analysis needs of the HPS experiment. The raw data collected over the running periods must be processed through calibration passes, reconstructed into calibrated objects useful for analysis and separated into analysis streams. Corresponding Monte Carlo will also need to be produced and separated into the same analysis streams.

Table~\ref{tab:data_size} shows the average trigger rate and the total amount of data expected over the 
different runs. For the six weeks long run in 2014 we expect to collect approximately 260~Tb of raw data.
\begin{table}[]
\centering
\begin{tabular}{|l|c|c|c|c|c|}
\hline
Run & Beam energy (GeV) & Time (days) & Raw data (Tb) & Processed data (Tb)\\
\hline
2014 & 1.1 & 14 & 49 & - \\
2014 & 2.2 & 14 & 45 & -  \\
2014 & 6.6 & 14 & 32 & -  \\
\hline
Total & - & 42 &  &  & \\
\hline
2015 & 1.1 & 30 & 106 & \\
2015 & 2.2 & 30 & 96 & \\
2015 & 6.6 & 30 & 69 & \\
\hline
Total & - & 90 & & \\
\hline
\end{tabular}
\caption{{\small Summary of the raw and processed data expected from the HPS runs. }}
\label{tab:data_size}
\end{table}
The raw data must be processed to produce physics data objects that can be analyzed. This reconstruction process will also include filters to select events of physics interest. The event size on disk is summarized 
in Table~\ref{tab:raw_data_size}. {\color{red} the reco should be smaller, it shouldn't include the raw data!?}
\begin{table}[]
\centering
\begin{tabular}{|l|c|}
\hline
Type & Event size (kB) \\ 
\hline
Raw (EVIO)  &  \\
\hline
Reconstructed LCIO & \\
\hline
\end{tabular}
\caption{{\small Data event sizes. }}
\label{tab:raw_data_size}
\end{table}

For modeling signals, estimating backgrounds and confirming the understanding of the detector 
performance, extensive Monte Carlo simulation is needed. Table~\ref{tab:mc_event_size} summarizes 
the typical event size at the various stages of the simulation. 
\begin{table}[]
\centering
\begin{tabular}{|lccc|}
\hline
Event type & Sim. stage & Size/500k bunches (Mb) & Mass points  \\
\hline 
A' signal & evgen & 22 & 10  \\
Beam bkg. (incl. brems. events) & evgen & 331 & -\\ 
Trident bkg. & evgen & 139 & - \\
\hline
Total & evgen & 431 & -\\
\hline
A' signal & Merged evgen & 134 & 10 \\
Beam bkg. & Merged evgen & 141 & - \\ 
Trident bkg. & Merged evgen & 134 & - \\
Beam+trident bkg. overlay & Merged evgen & 187 & - \\ 
A'+Beam+trident bkg. overlay & Merged evgen & 233 & 10 \\ 
\hline
Total & Merged evgen & 829 & - \\
\hline
A' signal & Reco. LCIO  & 284 & 10 \\
Beam bkg. & Reco. LCIO  & 366 & - \\ 
Trident bkg. & Reco. LCIO  & ? & - \\
Beam+trident bkg. overlay & Reco. LCIO  & 379 & - \\ 
A'+Beam+trident bkg. overlay & Reco. LCIO  & 383 & 10 \\ 
\hline
Total & Reco. LCIO  & 829 & - \\
\hline
\end{tabular}
\caption{{\small Event sizes per 500k simulated bunches. }}
\label{tab:mc_event_size}
\end{table}
%LCIO
%Beam bkg plus trident: 379 Mb per 500,000 bunches
%Beam: 366 Mb per 500,000 bunches
%AP/Beam/Trident: 383 Mb per 500,000 bunches at 7 different masses
%Ap: 284 Mb per 500,000
%
%Merged StdHep:
%Beam bkg plus trident: 187 Mb per 500,000 bunches
%Beam: 141 Mb per 500,000 bunches
%AP/Beam/Trident: 233 Mb per 500,000 bunches at 10 different masses
%Ap: 134 Mb per 500,000 bunches at 10 different ap masses
%Trident: 134 Mb per 500,000 bunches
%
%Raw StdHep: 
%Ap: 22 Mb per 500,000 bunches at 10 different ap masses
%Beam: 270 Mb per 500,000 bunches + an additional 61 Mb for brem events (500,000?)
%Trident: 139 Mb per 500,000 bunches
In total approximately 1/10th of the number of data events collected need to be simulated. 

% storage description
In total {\color{red} X (Y)~Tb} of storage for data is needed for the 2014 (2015) run. For simulation, taking 
into account multiple mass points for signal, this amounts to {\color{red} X} events and approximately 
{\color{red} Y~Tb}. Describe infrastructure needed to handle all this data. 
Disk space will be needed for ntuples, code releases, and scratch areas and approximately 10\% of the 
processed data. To accommodate this, 80 Tbytes will be needed. The HPS storage requirements are summarized in Tab.~\ref{tab:datastorage}.
\begin{table}[tbp]
\centering
\begin{tabular}{|l|c|}
\hline
\hline
\end{tabular}
\caption{{\small Data storage summary.}}
\label{tab:datastorage}
\end{table}


%cpu requirements
Approximately 0.1CPU seconds {\color{red} need update} are needed to reconstruct a data event on 
a typical 2.4~GHz core. This would require a total of {\color{red} Y} CPU hours of processing for the 
entires dataset and is available at the JLab processing center.  To simulate events, approximately 5 CPU seconds are needed for each event. In total {\color{red} Z} CPU hours is needed for Monte Carlo 
simulation which will be shared between SLAC and JLab. 
Based on experience with previous experiments, it is reasonable to estimate that the net CPU needed for 
analysis work (batch and interactive) will be comparable to that needed for production. We expect 
that this will be shared relatively evenly between JLab and SLAC.  
The HPS offline computing requirements are summarized in Tab.~\ref{tab:computing}.
\begin{table}[tbp]
\centering
\begin{tabular}{|l|c|}
\hline
\hline
\end{tabular}
\caption{{\small Computing needs summary.}}
\label{tab:computing}
\end{table}
