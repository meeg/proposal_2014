
The following is an outline of the offline computing model envisioned for satisfying the analysis needs of the HPS experiment. The raw data collected over the running periods must be processed through calibration passes, reconstructed into calibrated objects useful for analysis and separated into analysis streams. Corresponding Monte Carlo will also need to be produced and separated into the same analysis streams.

The raw data must be processed to produce physics data objects that can be analyzed. 
This reconstruction process will also include filters to select events of physics interest. We use the event size estimates 
in Table~\ref{tab:raw_data_size}, which are based on Table~\ref{tab:data_rates} from the previous section and object sizes 
in EVIO (raw data) and LCIO (processed data) formats. 
\begin{table}[ht]
\centering
\begin{tabular}{|l|c|c|}
\hline
Beam energy & Raw (EVIO) event size (kB) & Reconstructed (LCIO) event size (kB) \\ 
\hline
1.1 GeV  &  2.2 & 4.8 \\
2.2 GeV  &  2.3 & 5.0 \\
6.6 GeV  &  2.1 & 4.0 \\
\hline
\end{tabular}
\caption{{\small Data event sizes. }}
\label{tab:raw_data_size}
\end{table}

Table~\ref{tab:data_volume} shows the expected number of triggered events and
%the average trigger rate and 
the total amount of data expected over the 
different runs. 
We assume that the experiment collects data for all of its available beam time and the time allocated for detector commissioning, even though the experiment reach only assumes 50\% availability; this gives a conservative estimate of computing requirements. 
For trigger rate estimates, we use the ECal trigger rate from Section \ref{sec:ecaltrigg}; based on Appendix \ref{sec:muontrigg}, the muon system trigger rate is expected to be negligible.
\begin{table}[ht]
\centering
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
Run & $E_{beam}$ (GeV) & Time (days) & Events ($\times 10^9$) & Raw data (TB) & Processed data (TB)\\
\hline
2014 & 1.1 & 21 & 33 & 73 & 159 \\
2014 & 2.2 & 21 & 29 & 67 & 145  \\
\hline
Total & - & 42 & 62 & 140 & 304 \\
\hline
2015 & 2.2 & 35 & 48 & 112 & 241 \\
2015 & 6.6 & 35 & 38 & 80 & 153 \\
\hline
Total & - & 70 & 86 & 192 & 394 \\
\hline
\end{tabular}
\caption{{\small Summary of the raw and processed data expected from the HPS runs. }}
\label{tab:data_volume}
\end{table}


For modeling signals, estimating backgrounds and confirming the understanding of the detector 
performance, extensive Monte Carlo simulation is needed. 
Three types of events will be simulated: general beam background, trident background, and A' events.
% Table~\ref{tab:mc_event_size} summarizes 
% the typical event size at the various stages of the simulation. 
% \begin{table}[]
% \centering
% \begin{tabular}{|lccc|}
% \hline
% Event type & Sim. stage & Size/triggered event (kB) & Mass points  \\
% \hline
% Beam bkg.  & evgen	& 37.0	& 1	\\
% A' signal & evgen	& 0.5	& 10	\\
% A'+beam bkg & evgen	& 37.4	& 10	\\
% \hline
% Beam bkg.  & MC output	& 79.5	& 1	\\
% A' signal & MC output	& 2.5	& 10	\\
% A'+beam bkg & MC output	& 82.0	& 10	\\
% \hline
% \end{tabular}
% \caption{{\small Event sizes in kB per triggered event, including pileup events for beam background. }}
% \label{tab:mc_event_size}
% \end{table}

General beam background events will be generated by fully simulating beam background as described in \ref{sec:backgrounds} and simulating the HPS trigger. 
Because this is a compute-intensive process, only 1 million triggered events will be simulated at each beam energy; this is adequate for trigger and DAQ studies.

Trident background and A' events will be generated by using MadGraph to make trident or A' events 
with enhanced trigger probability, overlaying beam background, and simulating the trigger.
The amount of triggered trident events to be simulated is 10\% of the amount expected in actual data; 
the number of triggered A' events to be simulated is 100 million at each of 10 mass points at each beam energy. 
These will be used to test the analysis.

In total 472 (618)~TB of storage for data (raw, reconstructed and simulated) is needed for the 2014 (2015) run.
Tape is currently the only economical storage
solution for storing all of the raw, simulated and processed data.

The processing of the raw data is foreseen to occur at JLab. Given a
typical bandwidth between sites of 3 to 4 TB/day, 
only data summaries of events satisfying
pre-selection criteria for targeted analyses will be exported to remote
sites. Likewise, the size of the simulated data samples suggests that
the simulation should be processed and stored at JLAB and that 
only data summaries or small samples of the full data will be
exported.

Analyses needing
access to the hit level information will need to be run at JLab or run
on small samples of exported data unless they can take advantage of the
data summaries.

Data summaries will be written as ROOT trees. 65 (89) TB of DSTs will be generated 
in the 2014 (2015) run. These will be generated and stored on tape at JLab, and mirrored on tape at SLAC.

Disk space at JLAB will be 
needed for staging, code releases and scratch areas. 
Disk space will also be needed at SLAC for code releases and scratch areas. 
Both needs are covered by existing computing infrastructure.

The HPS storage requirements are summarized in Tab.~\ref{tab:datastorage}.
\begin{table}[ht]
\centering
\begin{tabular}{|l|c|c|}
\hline
Storage category & 2014 (TB) & 2015 (TB) \\
\hline
Raw data & 140 & 192 \\
Reconstructed data & 304 & 394 \\
Simulated data (raw and reconstructed) & 27 & 31 \\
\hline
Total data & 472 & 618 \\
\hline
DST (run data) & 62 & 86 \\
DST (simulated data) & 3 & 3 \\
\hline
Total DST  & 65  & 89 \\
\hline
\end{tabular}
\caption{{\small Data storage summary; data storage is at JLab only, while DST storage is common to JLab and SLAC.}}
\label{tab:datastorage}
\end{table}

% Approximately 0.015~CPU seconds are needed to reconstruct a data event on 
% a typical 2.4~GHz core. This would require a total of 0.26 million CPU hours of processing for the 
% entire 2014 dataset at the JLab processing center.  To simulate events, approximately 0.02 CPU seconds 
% are needed for a beam event and approximately 0.7 seconds for an A' event. In total 8.8 million CPU hours are needed for Monte Carlo 
% simulation for the 2014 run. 
% Based on experience with previous experiments, it is reasonable to estimate that the net CPU needed for 
% analysis work (batch and interactive) will be comparable to that needed for production. 
Simulation production and data reconstruction will be done on the batch farm at JLab. The CPU requirements 
are summarized in Tab.~\ref{tab:computing}.
\begin{table}[ht]
\centering
\begin{tabular}{|l|c|c|}
\hline
Computing category & 2014& 2015 \\
\hline
Raw data processing ($\times 10^{6}$~CPUh)  & 0.26 & 0.36 \\
Simulation production ($\times 10^{6}$~CPUh) & 0.84 & 0.99 \\
\hline
Total ($\times 10^{6}$~CPUh) & 1.10 & 1.35 \\
\hline
\end{tabular}
\caption{{\small Computing needs summary in CPU hours using typical 2.4~GHz cores.}}
\label{tab:computing}
\end{table}

The Jefferson Lab Computing Center provides computing and storage for experiments at JLab. 
A request will be submitted for data storage (tape and disk), computing resources (CPU hours for simulation and production), and data transfers to/from JLab.
