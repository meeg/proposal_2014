
The following is an outline of the offline computing model envisioned for satisfying the analysis needs of the HPS experiment. The raw data collected over the running periods must be processed through calibration passes, reconstructed into calibrated objects useful for analysis and separated into analysis streams. Corresponding Monte Carlo will also need to be produced and separated into the same analysis streams.

Table~\ref{tab:data_volume} shows the expected number of triggered events and
%the average trigger rate and 
the total amount of data expected over the 
different runs. For the six weeks long run in 2014 we expect to collect approximately 223~TB of raw data. 
We assume that the experiment collects data for all of its available beam time and the time allocated for detector commissioning, even though the experiment reach only assumes 50\% availability; this gives a conservative estimate of computing requirements.
\begin{table}[]
\centering
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
Run & $E_{beam}$ (GeV) & Time (days) & Events ($\times 10^9$) & Raw data (TB) & Processed data (TB)\\
\hline
2014 & 1.1 & 21 & 33 & 120 & 455 \\
2014 & 2.2 & 21 & 29 & 103 & 393  \\
%2014 & 6.6 & 14 & 15 & 55 & 209  \\
\hline
Total & - & 42 & 62 & 223  & 848 \\
\hline
%2015 & 1.1 & 30 & 47 & 171 & 650 \\
2015 & 2.2 & 35 & 48 & 172 & 655 \\
2015 & 6.6 & 35 & 38 & 137 & 522 \\
\hline
Total & - & 70 & 86 & 309 & 1177 \\
\hline
\end{tabular}
\caption{{\small Summary of the raw and processed data expected from the HPS runs. }}
\label{tab:data_volume}
\end{table}
The raw data must be processed to produce physics data objects that can be analyzed. This reconstruction process will also include filters to select events of physics interest. The event size on disk is summarized 
in Table~\ref{tab:raw_data_size}. 
\begin{table}[]
\centering
\begin{tabular}{|l|c|}
\hline
Type & Event size (kB) \\ 
\hline
Raw (EVIO)  &  3.6 \\
\hline
Reconstructed LCIO & 13.7 \\
\hline
\end{tabular}
\caption{{\small Data event sizes. }}
\label{tab:raw_data_size}
\end{table}

For modeling signals, estimating backgrounds and confirming the understanding of the detector 
performance, extensive Monte Carlo simulation is needed. Table~\ref{tab:mc_event_size} summarizes 
the typical event size at the various stages of the simulation. 
\begin{table}[]
\centering
\begin{tabular}{|lccc|}
\hline
Event type & Sim. stage & Size/triggered event (kB) & Mass points  \\
\hline
Beam bkg.  & evgen	& 37.0	& 1	\\
A' signal & evgen	& 0.5	& 10	\\
A'+beam bkg & evgen	& 37.4	& 10	\\
\hline
Beam bkg.  & MC output	& 79.5	& 1	\\
A' signal & MC output	& 2.5	& 10	\\
A'+beam bkg & MC output	& 82.0	& 10	\\
\hline
\end{tabular}
\caption{{\small Event sizes in kB per triggered event, including pileup events for beam background. }}
\label{tab:mc_event_size}
\end{table}

%\begin{table}[]
%\centering
%\begin{tabular}{|lccc|}
%\hline
%Event type & Sim. stage & Size/500k bunches (MB) & Mass points  \\
%\hline 
%A' signal & evgen & 22 & 10  \\
%Beam bkg. (incl. brems. events) & evgen & 331 & -\\ 
%Trident bkg. & evgen & 139 & - \\
%\hline
%Total & evgen & 431 & -\\
%\hline
%A' signal & Merged evgen & 134 & 10 \\
%Beam bkg. & Merged evgen & 141 & - \\ 
%Trident bkg. & Merged evgen & 134 & - \\
%Beam+trident bkg. overlay & Merged evgen & 187 & - \\ 
%A'+Beam+trident bkg. overlay & Merged evgen & 233 & 10 \\ 
%\hline
%Total & Merged evgen & 829 & - \\
%\hline
%A' signal & Reco. LCIO  & 284 & 10 \\
%Beam bkg. & Reco. LCIO  & 366 & - \\ 
%Trident bkg. & Reco. LCIO  & ???284??? & - \\
%Beam+trident bkg. overlay & Reco. LCIO  & 379 & - \\ 
%A'+Beam+trident bkg. overlay & Reco. LCIO  & 383 & 10 \\ 
%\hline
%Total & Reco. LCIO  & ??? & - \\
%\hline
%\end{tabular}
%\caption{{\small Event sizes in MB per 500k simulated bunches. }}
%\label{tab:mc_event_size}
%\end{table}
%LCIO
%Beam bkg plus trident: 379 Mb per 500,000 bunches
%Beam: 366 Mb per 500,000 bunches
%AP/Beam/Trident: 383 Mb per 500,000 bunches at 7 different masses
%Ap: 284 Mb per 500,000
%
%Merged StdHep:
%Beam bkg plus trident: 187 Mb per 500,000 bunches
%Beam: 141 Mb per 500,000 bunches
%AP/Beam/Trident: 233 Mb per 500,000 bunches at 10 different masses
%Ap: 134 Mb per 500,000 bunches at 10 different ap masses
%Trident: 134 Mb per 500,000 bunches
%
%Raw StdHep: 
%Ap: 22 Mb per 500,000 bunches at 10 different ap masses
%Beam: 270 Mb per 500,000 bunches + an additional 61 Mb for brem events (500,000?)
%Trident: 139 Mb per 500,000 bunches
In total approximately 1/10th of the number of data events collected need to be simulated. For the 2014 run this amounts to 6.2 billion events.
In addition, 100 million A' events must be simulated at each of 10 mass points at each beam energy. This totals 2 billion A' events.

% storage description
In total 2035 (2730)~Tb of storage for data is needed for the 2014 (2015) run.
%(17600+15800+12600)*86400*14/500000*383*1e6/1e12=42,6215
%                     #events/bunches*size in Mb*1M/1T= 

Tape is currently the only economical storage
solution for storing all of the raw, simulated and processed data.
The processing of the raw data is foreseen to occur at JLAB. Given a
typical bandwidth between sites of 3 to 4 Tb/day, analyses needing
access to the hit level information will need to be run at JLAB or run
on small samples of exported data unless they can take advantage of the
analysis streams. Analysis streams of events satisfying
pre-selections criteria for targeted analyses will be exported to remote
sites. Likewise, the simulated data is foreseen to be processed at the
remote sites and the equivalent analysis streams for the simulated data
will be used for sharing among the sites. In addition, disk space will be 
needed for ntuples, code releases, and scratch areas and approximately 10\% of the 
processed data. To accommodate this, ???80(171)~TB??? will be needed for the 2014 (2015) run. 
The HPS storage requirements are summarized in Tab.~\ref{tab:datastorage}.
\begin{table}[tbp]
\centering
\begin{tabular}{|l|c|c|}
\hline
Storage category & 2014 (TB) & 2015 (TB) \\
\hline
Raw data & 203 & 436 \\
Processed raw data & 774 & 1658 \\
Simulated data & 1025 & 1776 \\
\hline
Total tape space & 2035 & 2730 \\
\hline
%90days/42days*43Tb = 92Tb
Disk space  & ???80???  & ???171??? \\
\hline
\end{tabular}
\caption{{\small Data storage summary.}}
\label{tab:datastorage}
\end{table}
%(17600+15800+12600)*86400*14=55641600000 events
%(17600+15800+12600)*86400*14*0.1/3600}=1545600 cpu hours


%cpu requirements
Approximately 0.1~CPU seconds are needed to reconstruct a data event on 
a typical 2.4~GHz core. This would require a total of 1.7 million CPU hours of processing for the 
entire 2014 dataset at the JLab processing center.  To simulate events, approximately 0.02 CPU seconds 
are needed for a beam event and approximately 0.7 seconds for an A' event. In total 1.1 million CPU hours is needed for Monte Carlo 
simulation which will be shared between SLAC and JLab. 
Based on experience with previous experiments, it is reasonable to estimate that the net CPU needed for 
analysis work (batch and interactive) will be comparable to that needed for production. We expect 
that this will be shared relatively evenly between JLab and SLAC. The HPS offline computing requirements 
are summarized in Tab.~\ref{tab:computing}.
\begin{table}[tbp]
\centering
\begin{tabular}{|l|c|c|}
\hline
Computing category & 2014& 2015 \\
\hline
Raw data processing ($\times 10^{6}$~CPUh)  & 1.7 & 2.4 \\
Simulation production ($\times 10^{6}$~CPUh) & 1.1 & 1.1 \\
\hline
Total ($\times 10^{6}$~CPUh) & 2.8 & 3.5 \\
\hline
\end{tabular}
\caption{{\small Computing needs summary.}}
\label{tab:computing}
\end{table}
